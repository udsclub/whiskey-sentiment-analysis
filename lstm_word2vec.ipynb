{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# LSTM + word2vec\n",
    "\n",
    "#### Preprosessing\n",
    "\n",
    "nltk preprocessing, keras default tokenizer (remove stopwords, numbers, punctuation)\n",
    "\n",
    "#### Model\n",
    "\n",
    "LSTM base - one layer (100 - hidden units, without dropouts) \n",
    "* 1) word2vec train own texts\n",
    "* 2) pretrained word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"lstm_word2vec\"\n",
    "TRAIN_DATASETS = [\"data/test_imdb.csv\", \"data/train_imdb.csv\", \"data/test_rt_en.csv\", \"data/train_rt_en.csv\"]\n",
    "\n",
    "TOKENIZER_NAME = \"lstm_word2vec_tokenizer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import pickle\n",
    "import re\n",
    "numpy.random.seed(42)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Swith on full text mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for dataset in TRAIN_DATASETS:    \n",
    "    datasets.append(pd.read_csv(dataset, sep=\"|\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "whole_data = pd.concat(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n",
      "(40000, 3)\n",
      "(19798, 3)\n",
      "(79190, 3)\n"
     ]
    }
   ],
   "source": [
    "for data in datasets:\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data, train_data = train_test_split(whole_data, train_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14898, 5)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134090, 4)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Remove stopwords, numbers, punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "negatives = {\n",
    "    \"didn't\": \"didn_`_t\",\n",
    "    \"couldn't\": \"couldn_`_t\",\n",
    "    \"don't\": \"don_`_t\",\n",
    "    \"wouldn't\": \"wouldn_`_t\",\n",
    "    \"doesn't\": \"doesn_`_t\",\n",
    "    \"wasn't\": \"wasn_`_t\",\n",
    "    \"weren't\": \"weren_`_t\",\n",
    "    \"shouldn't\":\"shouldn_`_t\",\n",
    "    \"isn't\": \"isn_`_t\",\n",
    "    \"aren't\": \"aren_`_t\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('<br />', ' ')\n",
    "    text = ' '.join(tweet_tokenizer.tokenize(text))\n",
    "    for k, v in negatives.items():\n",
    "        text = text.replace(k, v)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a sentimental crowd-pleaser , well-directed by le mccarey , this tale about a priest ( bing crosby ) assigned to a problematic parish was so popular that paramount reteamed the same players for the bells of st . mary's .\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"\"\"A sentimental crowd-pleaser, well-directed by Le McCarey, this tale about a priest (Bing Crosby) assigned to a problematic parish was so popular that Paramount reteamed the same players for The Bells of St. Mary's.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenya/miniconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/Users/jenya/miniconda3/lib/python3.5/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "train_data['prep_text'] = train_data['text'].map(preprocess)\n",
    "test_data['prep_text'] = test_data['text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>prep_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13680</th>\n",
       "      <td>58770</td>\n",
       "      <td>1</td>\n",
       "      <td>Let's face it -- nothing short of a gorilla suit can make Peter MacNicol not look like a weenie.</td>\n",
       "      <td>let's face it - - nothing short of a gorilla suit can make peter macnicol not look like a weenie .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71716</th>\n",
       "      <td>22099</td>\n",
       "      <td>1</td>\n",
       "      <td>A modest and accessible Iranian film, softer than most but still intriguing.</td>\n",
       "      <td>a modest and accessible iranian film , softer than most but still intriguing .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>46279</td>\n",
       "      <td>1</td>\n",
       "      <td>I saw the movie and really could not stop my tears. Its tragedy that India has no such leaders after freedom, who dare to do justice with their own children, when they don't behave properly.. In current generation, politicians bring their children's into politics without measuring their caliber and skills.. I remember the dialogue from Gandhi 'What kind of society we want to create/make with such people (about Harilal)?' No wonder that it will be a dream that India will hardly have such leader in this or next generation.. Einstein was right when he said about Gandhi that 'After 50 years one would hardly believe that such person with body, soul and mind (Mahatma Gandhi) had ever lived on this earth.' I sincerely want to THANKS a LOT to Anil kapoor, Feroze khan and all film actors/actresses for this wonderful movie about great person and relationship with his son. All father and son should watch this movie once and take some lessons for both roles.</td>\n",
       "      <td>i saw the movie and really could not stop my tears . its tragedy that india has no such leaders after freedom , who dare to do justice with their own children , when they don_`_t behave properly .. in current generation , politicians bring their children's into politics without measuring their caliber and skills .. i remember the dialogue from gandhi ' what kind of society we want to create / make with such people ( about harilal ) ? ' no wonder that it will be a dream that india will hardly have such leader in this or next generation .. einstein was right when he said about gandhi that ' after 50 years one would hardly believe that such person with body , soul and mind ( mahatma gandhi ) had ever lived on this earth . ' i sincerely want to thanks a lot to anil kapoor , feroze khan and all film actors / actresses for this wonderful movie about great person and relationship with his son . all father and son should watch this movie once and take some lessons for both roles .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25816</th>\n",
       "      <td>60370</td>\n",
       "      <td>0</td>\n",
       "      <td>Despite a rich premise, Just Friends ultimately fails to live up to its early potential, settling into broad comedy that shortchanges the romantic possibilities.</td>\n",
       "      <td>despite a rich premise , just friends ultimately fails to live up to its early potential , settling into broad comedy that shortchanges the romantic possibilities .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22921</th>\n",
       "      <td>9272</td>\n",
       "      <td>1</td>\n",
       "      <td>Shot in astonishingly elaborate long takes, this is the kind of film that finds the most brilliant poetry in the slightest movement of the camera -- a paradigm of cinematic expression.</td>\n",
       "      <td>shot in astonishingly elaborate long takes , this is the kind of film that finds the most brilliant poetry in the slightest movement of the camera - - a paradigm of cinematic expression .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  label  \\\n",
       "13680  58770       1       \n",
       "71716  22099       1       \n",
       "2273   46279       1       \n",
       "25816  60370       0       \n",
       "22921  9272        1       \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text  \\\n",
       "13680  Let's face it -- nothing short of a gorilla suit can make Peter MacNicol not look like a weenie.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "71716  A modest and accessible Iranian film, softer than most but still intriguing.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "2273   I saw the movie and really could not stop my tears. Its tragedy that India has no such leaders after freedom, who dare to do justice with their own children, when they don't behave properly.. In current generation, politicians bring their children's into politics without measuring their caliber and skills.. I remember the dialogue from Gandhi 'What kind of society we want to create/make with such people (about Harilal)?' No wonder that it will be a dream that India will hardly have such leader in this or next generation.. Einstein was right when he said about Gandhi that 'After 50 years one would hardly believe that such person with body, soul and mind (Mahatma Gandhi) had ever lived on this earth.' I sincerely want to THANKS a LOT to Anil kapoor, Feroze khan and all film actors/actresses for this wonderful movie about great person and relationship with his son. All father and son should watch this movie once and take some lessons for both roles.   \n",
       "25816  Despite a rich premise, Just Friends ultimately fails to live up to its early potential, settling into broad comedy that shortchanges the romantic possibilities.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "22921  Shot in astonishingly elaborate long takes, this is the kind of film that finds the most brilliant poetry in the slightest movement of the camera -- a paradigm of cinematic expression.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         prep_text  \n",
       "13680  let's face it - - nothing short of a gorilla suit can make peter macnicol not look like a weenie .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "71716  a modest and accessible iranian film , softer than most but still intriguing .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "2273   i saw the movie and really could not stop my tears . its tragedy that india has no such leaders after freedom , who dare to do justice with their own children , when they don_`_t behave properly .. in current generation , politicians bring their children's into politics without measuring their caliber and skills .. i remember the dialogue from gandhi ' what kind of society we want to create / make with such people ( about harilal ) ? ' no wonder that it will be a dream that india will hardly have such leader in this or next generation .. einstein was right when he said about gandhi that ' after 50 years one would hardly believe that such person with body , soul and mind ( mahatma gandhi ) had ever lived on this earth . ' i sincerely want to thanks a lot to anil kapoor , feroze khan and all film actors / actresses for this wonderful movie about great person and relationship with his son . all father and son should watch this movie once and take some lessons for both roles .  \n",
       "25816  despite a rich premise , just friends ultimately fails to live up to its early potential , settling into broad comedy that shortchanges the romantic possibilities .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "22921  shot in astonishingly elaborate long takes , this is the kind of film that finds the most brilliant poetry in the slightest movement of the camera - - a paradigm of cinematic expression .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Padding data\n",
    "\n",
    "Keras [Embedding layer](https://keras.io/layers/embeddings/) turn positive integers (indexes) into dense vectors of fixed size. \n",
    "\n",
    "* 1) Firstly convert words to indexes\n",
    "* 2) Then we padding data\n",
    "\n",
    "['юристы есть', 'мне нужны юристы'] -> [[0, 4, 10], [2, 3, 4]] -> [[0.25, 0.1], [0.6, -0.2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Loading google word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded\n",
      "CPU times: user 2min 33s, sys: 5.44 s, total: 2min 38s\n",
      "Wall time: 2min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_google = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "print(\"Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 6s, sys: 14.8 s, total: 1min 21s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Normalizing word2vec vectors.\n",
    "word2vec_google.init_sims(replace=True)  # Normalizes the vectors in the word2vec class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec_google.index2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 1. String -> Int vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 117811 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS, filters='\"#$%&()*+-/:;<=>@[\\\\]^{|}~\\t\\n,.')\n",
    "tokenizer.fit_on_texts(train_data['prep_text'])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#save tokenizer\n",
    "with open(TOKENIZER_NAME,'wb') as ofile:\n",
    "    pickle.dump(tokenizer, ofile)\n",
    "    ofile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_embedding(word2vec_model, word):\n",
    "    try:\n",
    "        return word2vec_model[word]\n",
    "    except KeyError:\n",
    "        return np.zeros(word2vec_model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "    \n",
    "embedding_weights_google = np.zeros((nb_words, word2vec_google.vector_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_weights_google[i] = get_embedding(word2vec_google, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2. Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sequences_train = tokenizer.texts_to_sequences(train_data['prep_text'])\n",
    "sequences_test = tokenizer.texts_to_sequences(test_data['prep_text'])\n",
    "\n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def padding(text):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_train = padded_sequences_train\n",
    "x_test = padded_sequences_test\n",
    "y_train = train_data['label']\n",
    "y_test= test_data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "LSTM_DIM = 128\n",
    "EMBEDDING_DIM = 300\n",
    "DROPOUT_U = 0.2\n",
    "DROPOUT_W = 0.2\n",
    "DROPOUT_AFTER_LSTM = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Flatten, Dropout\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (134090, 70)\n",
      "Shape of label tensor: (134090, 2)\n"
     ]
    }
   ],
   "source": [
    "labels = to_categorical(np.asarray(train_data['label']))\n",
    "print('Shape of data tensor:', padded_sequences_train.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_model(pretrained_embedding_weights = None):\n",
    "    model = Sequential()\n",
    "    if pretrained_embedding_weights is not None:\n",
    "        model.add(Embedding(nb_words,\n",
    "                            EMBEDDING_DIM, \n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False,\n",
    "                            weights=[pretrained_embedding_weights]))\n",
    "    else:\n",
    "        model.add(Embedding(n_symbols, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "    #model.add(LSTM(LSTM_DIM, dropout_U=DROPOUT_U, dropout_W=DROPOUT_W))\n",
    "    model.add(Bidirectional(LSTM(LSTM_DIM, dropout_U=DROPOUT_U, dropout_W=DROPOUT_W)))\n",
    "    model.add(Dropout(DROPOUT_AFTER_LSTM))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    metrics=['accuracy', 'fmeasure', 'precision', 'recall']\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstm_word2vec_google = create_model(embedding_weights_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 70, 300)       6000000     embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 256)           439296      embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 256)           0           bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1)             257         dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 6,439,553\n",
      "Trainable params: 439,553\n",
      "Non-trainable params: 6,000,000\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_word2vec_google.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tensor_board = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=False, write_images=False)\n",
    "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model_checkpoint = ModelCheckpoint(\"models/%s.hdf5\" % MODEL_NAME, monitor='val_acc', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 134090 samples, validate on 14898 samples\n",
      "WARNING:tensorflow:From /Users/jenya/miniconda3/lib/python3.5/site-packages/keras/callbacks.py:618 in set_model.: merge_all_summaries (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge_all.\n",
      "Epoch 1/50\n",
      "Epoch 00000: val_acc improved from -inf to 0.77077, saving model to models/rt_lstm_word2vec.hdf5\n",
      "1377s - loss: 0.5319 - acc: 0.7284 - fmeasure: 0.7764 - precision: 0.7535 - recall: 0.8088 - val_loss: 0.4735 - val_acc: 0.7708 - val_fmeasure: 0.7985 - val_precision: 0.8322 - val_recall: 0.7693\n",
      "Epoch 2/50\n",
      "Epoch 00001: val_acc improved from 0.77077 to 0.78870, saving model to models/rt_lstm_word2vec.hdf5\n",
      "1063s - loss: 0.4761 - acc: 0.7658 - fmeasure: 0.8040 - precision: 0.7869 - recall: 0.8260 - val_loss: 0.4393 - val_acc: 0.7887 - val_fmeasure: 0.8217 - val_precision: 0.8200 - val_recall: 0.8253\n",
      "Epoch 3/50\n",
      "Epoch 00002: val_acc improved from 0.78870 to 0.79628, saving model to models/rt_lstm_word2vec.hdf5\n",
      "982s - loss: 0.4461 - acc: 0.7854 - fmeasure: 0.8195 - precision: 0.8054 - recall: 0.8372 - val_loss: 0.4258 - val_acc: 0.7963 - val_fmeasure: 0.8273 - val_precision: 0.8305 - val_recall: 0.8260\n",
      "Epoch 4/50\n",
      "Epoch 00003: val_acc improved from 0.79628 to 0.80460, saving model to models/rt_lstm_word2vec.hdf5\n",
      "950s - loss: 0.4258 - acc: 0.7978 - fmeasure: 0.8293 - precision: 0.8183 - recall: 0.8438 - val_loss: 0.4059 - val_acc: 0.8046 - val_fmeasure: 0.8404 - val_precision: 0.8138 - val_recall: 0.8706\n",
      "Epoch 5/50\n",
      "Epoch 00004: val_acc improved from 0.80460 to 0.81346, saving model to models/rt_lstm_word2vec.hdf5\n",
      "958s - loss: 0.4098 - acc: 0.8050 - fmeasure: 0.8347 - precision: 0.8265 - recall: 0.8459 - val_loss: 0.3952 - val_acc: 0.8135 - val_fmeasure: 0.8446 - val_precision: 0.8324 - val_recall: 0.8591\n",
      "Epoch 6/50\n",
      "Epoch 00005: val_acc improved from 0.81346 to 0.81971, saving model to models/rt_lstm_word2vec.hdf5\n",
      "1010s - loss: 0.3985 - acc: 0.8132 - fmeasure: 0.8412 - precision: 0.8346 - recall: 0.8505 - val_loss: 0.3879 - val_acc: 0.8197 - val_fmeasure: 0.8462 - val_precision: 0.8533 - val_recall: 0.8407\n",
      "Epoch 7/50\n",
      "Epoch 00006: val_acc improved from 0.81971 to 0.82152, saving model to models/rt_lstm_word2vec.hdf5\n",
      "1040s - loss: 0.3871 - acc: 0.8209 - fmeasure: 0.8479 - precision: 0.8414 - recall: 0.8571 - val_loss: 0.3841 - val_acc: 0.8215 - val_fmeasure: 0.8547 - val_precision: 0.8240 - val_recall: 0.8895\n",
      "Epoch 8/50\n",
      "Epoch 00007: val_acc improved from 0.82152 to 0.82857, saving model to models/rt_lstm_word2vec.hdf5\n",
      "1348s - loss: 0.3765 - acc: 0.8265 - fmeasure: 0.8527 - precision: 0.8458 - recall: 0.8621 - val_loss: 0.3706 - val_acc: 0.8286 - val_fmeasure: 0.8583 - val_precision: 0.8392 - val_recall: 0.8799\n",
      "Epoch 9/50\n",
      "Epoch 00008: val_acc improved from 0.82857 to 0.83387, saving model to models/rt_lstm_word2vec.hdf5\n",
      "1766s - loss: 0.3682 - acc: 0.8303 - fmeasure: 0.8556 - precision: 0.8496 - recall: 0.8640 - val_loss: 0.3684 - val_acc: 0.8339 - val_fmeasure: 0.8613 - val_precision: 0.8504 - val_recall: 0.8745\n",
      "Epoch 10/50\n",
      "Epoch 00009: val_acc did not improve\n",
      "1032s - loss: 0.3594 - acc: 0.8355 - fmeasure: 0.8599 - precision: 0.8547 - recall: 0.8675 - val_loss: 0.3709 - val_acc: 0.8315 - val_fmeasure: 0.8534 - val_precision: 0.8785 - val_recall: 0.8315\n",
      "Epoch 11/50\n",
      "Epoch 00010: val_acc improved from 0.83387 to 0.83649, saving model to models/rt_lstm_word2vec.hdf5\n",
      "1011s - loss: 0.3521 - acc: 0.8395 - fmeasure: 0.8632 - precision: 0.8585 - recall: 0.8701 - val_loss: 0.3606 - val_acc: 0.8365 - val_fmeasure: 0.8618 - val_precision: 0.8612 - val_recall: 0.8642\n",
      "Epoch 12/50\n",
      "Epoch 00011: val_acc improved from 0.83649 to 0.83749, saving model to models/rt_lstm_word2vec.hdf5\n",
      "1038s - loss: 0.3437 - acc: 0.8445 - fmeasure: 0.8675 - precision: 0.8627 - recall: 0.8742 - val_loss: 0.3561 - val_acc: 0.8375 - val_fmeasure: 0.8636 - val_precision: 0.8569 - val_recall: 0.8722\n",
      "Epoch 13/50\n",
      "Epoch 00012: val_acc did not improve\n",
      "4854s - loss: 0.3362 - acc: 0.8485 - fmeasure: 0.8710 - precision: 0.8654 - recall: 0.8787 - val_loss: 0.3570 - val_acc: 0.8367 - val_fmeasure: 0.8593 - val_precision: 0.8745 - val_recall: 0.8465\n",
      "Epoch 14/50\n",
      "Epoch 00013: val_acc did not improve\n",
      "1091s - loss: 0.3313 - acc: 0.8513 - fmeasure: 0.8733 - precision: 0.8683 - recall: 0.8805 - val_loss: 0.3573 - val_acc: 0.8375 - val_fmeasure: 0.8600 - val_precision: 0.8763 - val_recall: 0.8463\n",
      "Epoch 15/50\n",
      "Epoch 00014: val_acc improved from 0.83749 to 0.83843, saving model to models/rt_lstm_word2vec.hdf5\n",
      "1095s - loss: 0.3248 - acc: 0.8538 - fmeasure: 0.8753 - precision: 0.8713 - recall: 0.8813 - val_loss: 0.3542 - val_acc: 0.8384 - val_fmeasure: 0.8674 - val_precision: 0.8430 - val_recall: 0.8951\n",
      "Epoch 16/50\n",
      "Epoch 00015: val_acc improved from 0.83843 to 0.83864, saving model to models/rt_lstm_word2vec.hdf5\n",
      "1088s - loss: 0.3175 - acc: 0.8596 - fmeasure: 0.8804 - precision: 0.8758 - recall: 0.8869 - val_loss: 0.3569 - val_acc: 0.8386 - val_fmeasure: 0.8598 - val_precision: 0.8833 - val_recall: 0.8393\n",
      "Epoch 17/50\n",
      "Epoch 00016: val_acc improved from 0.83864 to 0.83978, saving model to models/rt_lstm_word2vec.hdf5\n",
      "1086s - loss: 0.3116 - acc: 0.8615 - fmeasure: 0.8819 - precision: 0.8772 - recall: 0.8886 - val_loss: 0.3563 - val_acc: 0.8398 - val_fmeasure: 0.8640 - val_precision: 0.8673 - val_recall: 0.8624\n",
      "Epoch 18/50\n",
      "Epoch 00017: val_acc improved from 0.83978 to 0.84045, saving model to models/rt_lstm_word2vec.hdf5\n",
      "1157s - loss: 0.3054 - acc: 0.8643 - fmeasure: 0.8843 - precision: 0.8800 - recall: 0.8904 - val_loss: 0.3592 - val_acc: 0.8404 - val_fmeasure: 0.8633 - val_precision: 0.8756 - val_recall: 0.8532\n",
      "Epoch 19/50\n",
      "Epoch 00018: val_acc improved from 0.84045 to 0.84099, saving model to models/rt_lstm_word2vec.hdf5\n",
      "1059s - loss: 0.2993 - acc: 0.8674 - fmeasure: 0.8869 - precision: 0.8833 - recall: 0.8923 - val_loss: 0.3543 - val_acc: 0.8410 - val_fmeasure: 0.8637 - val_precision: 0.8748 - val_recall: 0.8546\n",
      "Epoch 20/50\n",
      "Epoch 00019: val_acc did not improve\n",
      "1060s - loss: 0.2940 - acc: 0.8707 - fmeasure: 0.8897 - precision: 0.8854 - recall: 0.8957 - val_loss: 0.3568 - val_acc: 0.8409 - val_fmeasure: 0.8661 - val_precision: 0.8618 - val_recall: 0.8720\n",
      "Epoch 21/50\n",
      "Epoch 00020: val_acc improved from 0.84099 to 0.84374, saving model to models/rt_lstm_word2vec.hdf5\n",
      "1068s - loss: 0.2887 - acc: 0.8728 - fmeasure: 0.8916 - precision: 0.8877 - recall: 0.8972 - val_loss: 0.3609 - val_acc: 0.8437 - val_fmeasure: 0.8695 - val_precision: 0.8594 - val_recall: 0.8817\n",
      "Epoch 22/50\n",
      "Epoch 00021: val_acc improved from 0.84374 to 0.84508, saving model to models/rt_lstm_word2vec.hdf5\n",
      "1059s - loss: 0.2831 - acc: 0.8774 - fmeasure: 0.8953 - precision: 0.8915 - recall: 0.9007 - val_loss: 0.3580 - val_acc: 0.8451 - val_fmeasure: 0.8697 - val_precision: 0.8652 - val_recall: 0.8760\n",
      "Epoch 23/50\n",
      "Epoch 00022: val_acc did not improve\n",
      "1091s - loss: 0.2760 - acc: 0.8790 - fmeasure: 0.8966 - precision: 0.8937 - recall: 0.9009 - val_loss: 0.3592 - val_acc: 0.8427 - val_fmeasure: 0.8668 - val_precision: 0.8678 - val_recall: 0.8676\n",
      "Epoch 24/50\n",
      "Epoch 00023: val_acc did not improve\n",
      "1046s - loss: 0.2715 - acc: 0.8813 - fmeasure: 0.8986 - precision: 0.8957 - recall: 0.9030 - val_loss: 0.3637 - val_acc: 0.8423 - val_fmeasure: 0.8660 - val_precision: 0.8710 - val_recall: 0.8628\n",
      "Epoch 25/50\n",
      "Epoch 00024: val_acc did not improve\n",
      "1041s - loss: 0.2666 - acc: 0.8833 - fmeasure: 0.9002 - precision: 0.8980 - recall: 0.9039 - val_loss: 0.3722 - val_acc: 0.8431 - val_fmeasure: 0.8692 - val_precision: 0.8569 - val_recall: 0.8833\n",
      "Epoch 26/50\n",
      "Epoch 00025: val_acc did not improve\n",
      "1164s - loss: 0.2606 - acc: 0.8864 - fmeasure: 0.9029 - precision: 0.9001 - recall: 0.9070 - val_loss: 0.3730 - val_acc: 0.8423 - val_fmeasure: 0.8662 - val_precision: 0.8685 - val_recall: 0.8654\n",
      "CPU times: user 1d 19h 4min 50s, sys: 8h 4min 41s, total: 2d 3h 9min 32s\n",
      "Wall time: 9h 2min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x291c33dd8>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lstm_word2vec_google.fit(x_train, y_train, \n",
    "                         nb_epoch=50,\n",
    "                         batch_size=128,\n",
    "                         verbose=2,\n",
    "                         validation_data=(x_test, y_test),\n",
    "                         callbacks=[tensor_board, early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = lstm_word2vec_google.predict(padded_sequences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data['p'] = [p[0] for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data[test_data['p'] < 0.1][test_data['label'] == 1][['label', 'p', 'text']].sort_values('p')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data[test_data['p'] > 0.9][test_data['label'] == 0][['label', 'p', 'text']].sort_values('p', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lstm_predict(text):\n",
    "    return lstm_word2vec_google.predict(padding(preprocess(text)))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.042715129"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_predict('good actors, but bad movie ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.092992492"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_predict('really good actors, but bad movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.042997032"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_predict('good actors, but really bad movie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86298966"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_predict(\"i think the movie is good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20842142"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_predict(\"i don't think the movie good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82579565"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_predict(\"i doubt the movie is good\") ## bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "lstm_word2vec_google_json = lstm_word2vec_google.to_json()\n",
    "with open(MODEL_NAME + \".json\", \"w\") as json_file:\n",
    "    json_file.write(lstm_word2vec_google_json)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
